{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data cleaning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKtay1im24xF4x1tpLKKa4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mosaddeq/Sentiment-Analysis/blob/master/data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKTXRsIcaSS2"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import drive\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import string\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "data =pd.read_csv('/content/drive/My Drive/Colab Notebooks/Thesis Data test.csv', encoding='utf8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRNKVb6Q-UUi",
        "outputId": "caf69baf-1ba8-47a4-eb64-2ae07056dafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdV_KLX2YX65"
      },
      "source": [
        "#punctuations removing  \n",
        "def remove_punctuation(txt):\n",
        "  text_nopunct = \"\".join([p for p in txt if p not in string.punctuation])\n",
        "  return text_nopunct"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbVVA0sQZH8l"
      },
      "source": [
        "data['Sentence_cleaned'] = data['Sentence'].apply(lambda x: remove_punctuation(x))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVWmElczvRI9"
      },
      "source": [
        "#tokenizing\n",
        "import re\n",
        "def tokenize(txt):\n",
        "  tokens = re.split('\\W+', txt)\n",
        "  return tokens\n",
        "data['Sentence_cleaned'] = data['Sentence_cleaned'].apply(lambda x: tokenize(x.lower()))\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwwXIGWC3C_o"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwbrBZCI4IBk"
      },
      "source": [
        "#removing stopwords\n",
        "def remove_stopwords(txt_tokenized):\n",
        "  txt_clean = [word for word in txt_tokenized if word not in stopwords]\n",
        "  return txt_clean\n",
        "data['Sentence_cleaned'] = data['Sentence_cleaned'].apply(lambda x: remove_stopwords(x))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCemLT2P8EGE"
      },
      "source": [
        "pd.set_option('display.max_colwidth',100)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG8CDGSrB1FM"
      },
      "source": [
        "wn= nltk.WordNetLemmatizer()\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7XRrD-lDnkR"
      },
      "source": [
        "def lemmatizing(tokenized_text):\n",
        "  text= [wn.lemmatize(word) for word in tokenized_text]\n",
        "  return text\n",
        "data['Sentence_cleaned'] = data['Sentence_cleaned'].apply(lambda x: lemmatizing(x))\n",
        "data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgdv5obCER59"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}